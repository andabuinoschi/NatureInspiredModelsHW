\documentclass{article}
\usepackage[utf8]{inputenc}

\title{Solving Min-Max MTSP using Evolutive Algorithms and Clustering}
\author{Anda Buinoschi MOC2, Bogdan Lunca»ôu MOC1 }
\date{}

\usepackage{natbib}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{amsmath}

\begin{document}

\maketitle

\begin{abstract}
This paper showcases a method for solving Min-Max Multiple Traveling Salesman making use of Ant Colony Optimization, Simulated Annealing and K-means clustering algorithm. This method proves comparable results to one of the available benchmarks. Min Max MTSP is a NP hard problem in which apart from the classical problem, it is needed to minimize the maximum tours for each salesman, balancing the cost among the tours. In the upcoming sections we introduce the problem, describe the methods, present the experiments' results and compare them with the benchmark.
\end{abstract}

\section{Introduction}
Traveling Salesman Problem is one of the most studied problem in combinatorial optimization, due to the complexity of finding good solutions for a large amount of cities in a timely manner. It states that: a traveling salesman has to visit a number of given cities in the shortest tour distance possible, visiting each city exactly once. The general case is MTSP in which you have more salesmen and it is needed to find subtours for them who serve a set of cities. Being in the class of NP hard problems, MTSP solution time grows exponentially with the increase in input data points. As a result, the classical optimization procedures are not adequate for this problem especially for large dimensions. In other words, as the size of the problem increases, the complexity of the problem also increases rapidly as well. We present a method for this problem by combining clustering methods with combinatorial optimization algorithms.

\section{Methods}
\subsection{Algorithms}
\subsubsection{K Means}
K means algorithm is a method used for clustering a set of points with a label according to a similarity measure. Usually, if the data points are in an Euclidian space, then it is used their distance as similarity measure.\\
Given a set of data points $D=\{x_1, x_2,...,x_n\}$, the algorithm iteratively search for an optimal centroid for $k$ clusters, the latter being given as parameter. The $k$ centroids are chosen initially randomly from the search space or from the data points given. Assigning each point to their clusters according to the closest centroid, the latter are adjusted by recomputing their position, converging to an optimal solution in the end.
\subsubsection{Simulated annealing}
Simulated Annealing algorithm is a popular metaheuristic algorithm used for solving discrete and continuous optimization problems. The key feature lies in finding the right parameters for temperature, the cooling system and $\alpha$ which is used in the cooling function. Its origins are in statistical mechanics and its fundamental idea is to accept moves resulting in worse quality solutions than the current one to escape from local optima. The probability of accepting such a move is decreased through the temperature parameter. The Metropolis acceptance criterion, which models how a thermodynamic system moves from one state to other in which the energy is minimized, is used for accepting a worse neighbourhood or not. In order to apply the algorithm to a specific problem, it is needed to define a neighbourhood structure an the cooling schedule.\\
The cooling schedule defines the way in which the temperature is going to be decreased and it is crucial for the success of the search. A very low cooling schedule will take too many iterations to reach the global optima and if the number of iterations is limited, then this can lead to an unsuccessful result. On the other hand, a too fast cooling schedule can get the algorithm trapped in a local optima. 
The most common cooling schedule is geometric cooling schedule, which can be described by the formula $T_{k+1} = \alpha \cdot T_{k}$, where $T$ is the temperature value, $k$ is considered the iteration and $alpha$ is the cooling rate. In this type of function, $alpha$ has to be smaller but close to 1, otherwise this would lead to an accelerated cooling system. Usually, for this function $alpha$ is choosing from the interval $[0.8, 0.99]$. Another cooling schedule is the logarithmic schedule $T_{k+1} = \frac{\alpha T_k}{ln(1 + k)}$. Its convergence to reach the global optima is proved for $\alpha = 0$ but it is such a slow cooling schedule that is rarely used in practice. Even though smaller values for $alpha$ can speed it up, it is considered to be slow in general.
The pseudocode for the algorithm can be found below:
\begin{algorithm}[H]
\SetAlgoLined
\SetKwRepeat{Repeat}{repeat}{until}
 $t = 0$\;
 initialize temperature T\;
 select a current candidate solution (bitstring) $v_c$ at random\;
 evaluate $v_c$\;
 \Repeat{(halting-criterion)}
 {
    \Repeat{(termination-condition)}
    {
        select at random $v_n$ - a neighbour of $v_c$\;
        \uIf {eval($v_n$) is better than eval($v_c$)}
        {
            $v_c = v_n$\;
        }
        \uElseIf{random([0,1)) $< exp(-\frac{\mid{eval(v_n) - eval(v_c)}\mid}{T})$}
        {
            $v_c = v_n$\;
        }
    }
    $T = g(T,t)$\;
    $t = t + 1$\;
 }
\caption{Simulated Annealing algorithm}
\end{algorithm}
For our paper we used geometric cooling schedule and for the neighbourhood structure we chose 2opt method, used for discrete values.
\subsubsection{Ant Colony Optimization}
The Ant Colony Optimization is very popular among the combinatorial problems on graph structures because it was inspired by some experiments on ant species which were trying to find the shortest path from their nest to the place where they could find food through a process called stigmergy. This process defines the environmental changes through the pheromone they were leaving on their way in such manner that the other ants from the colony were attracted to go on the same path since this is their communication method.
The pseudocode in a short brief is the following:\\
\begin{algorithm}[H]
\SetAlgoLined
    set parameters, initialize pheromone trails\;
    \While{termination condition not met}
    {
        ConstructAntsSolutions\;
        ApplyLocalSearch\;
        UpdatePheromones\;
    }
 \caption{Ant Colony Optimization algorithm (short presentation)}
\end{algorithm}
The main phases of the algorithm constitute the ants' solution construction and the pheromone update. Each ant $k$ from the colony has the following properties: it exploits the construction graph in the search for optimal solutions, it has a memory about the path it followed, a starting state and an ending state, selects a move by applying a probabilistic decision rule and updates the pheromone trail associated with the used components. Each ant acts concurrently and independently and that although each ant is complex enough to find a probably poor solution, the good solutions can only emerge as the result of the collective interaction within the colony.\\
In the algorithm, each ant concurrently builds a tour. They start from a node and for the construction step, each of them applies a probabilistic action choice rule to decide which node to visit next. In particular, the probability to visit node $j$ from node $i$ is the following:
$$p_{ij} = \frac{[\ \tau_{ij} ] \ ^\alpha \cdot [\ \eta_{ij}]\ ^\beta}{\sum_{l \in N_i} [\ \tau_{il} ] \ ^\alpha \cdot [\ \eta_{il}]\ ^\beta}, if j \in N_i,$$
where $\eta_{ij} = \frac{1}{d_{ij}}$ is a heuristic value that is available a priori, $\alpha$ and $\beta$ are parameters which determine the relative influence of the pheromone trail and the heuristic information, and $N_i$ represent the feasible neighbourhood of the ant when being in node $i$; the probability of choosing a node outside $N_i$ is set to 0. If $\alpha = 0$ then the closest cities are more likely to be visited, corresponding to a stochastic greedy algorithm. If $\beta = 0$, only pheromone amplification will be at work. This generally leads to poor results and for values $\alpha > 1$ it leads to rapid emergence and stagnation, a situation in which all the ants are following the same path and construct the same tour. Usually, $\alpha$ is set to 1 and $\beta$ ranges from 2 to 5.\\
After the ants constructed their tour, the pheromone trails are updated. This step consists in pheromone evaporation (in order to avoid the accumulation of the pheromone trails and "forget" bad decisions previously taken in the exploration step) and then add the pheromone on the arcs the ants have crossed in their tours. Pheromone evaporation is define by 
$$\tau_{ij} = (1 - \rho) \tau_{ij}, \forall (i,j) \in L,$$
where $L$ is the set of arcs that connect the nodes of the graph in the problem and $0 < \rho \leq 1$ is the pheromone evaporation rate. After evaporation step, all ants deposit pheromone on the arcs they have crossed in their tour:
$$\tau_{ij} = \tau_{ij} + \sum_{k = 1} ^ m \Delta\tau_{ij}^k, \forall (i,j) \in L,$$
where $\Delta\tau_{ij}^k$ is the amount of pheromone ant $k$ deposits on the arcs it has visited. It is defined by:
$$\Delta\tau_{ij}^k = 
                     \begin{cases} 
                          1/C^k & $if arc $(i,j)$ belongs to $T^k \\
                          0 & otherwise 
                       \end{cases},
$$
where $C^k$, the length of the tour $T^k$ built by the k-th ant, is computed as the sum of the lengths of the arcs belonging to $T^k$. This means that the better an ant's tour is, the more pheromone the arcs belonging to this tour receive. In general, arcs that are used by many ants and which are part of short tours, receive more pheromone and therefore more likely to be chosen by ants in future iterations of the algorithm.
\subsection{Our method}
Our method consisted in running KMeans clustering algorithm, with the parameter $k$ set to the number of travel salesman that our problem has, then running Simulated Annealing and Ant Colony Optimization on these resulted clusters. The idea of creating these clusters was to find neighbourhoods and nodes from which each of the travel salesmen start creating their tours. We excluded the first point of the dataset (considered to be depot) while creating the clusters, and then starting constructing the tours with the depot point. These neighbourhoods have the benefit of being conglomerate and they can lead to good results (short tour costs within clusters), almost comparable to the benchmark.
\section{Experiments}
The experiments were run on the following datasets: \emph{eil51}, \emph{berlin52}, \emph{eil76}, \emph{rat99} which can be accessible on TSPLib.
\subsection{Simulated Annealing with clustering}
For the simulated annealing the following best parameters were chosen:
\begin{itemize}
  \item temperature T =
  \item cooling rate $\alpha$ = 
  \item geometric cooling schedule
\end{itemize}

\begin{table}[h!]
\centering
\begin{tabular}{ |c|c|c|c|c| } 
\hline
Instance Name & n & m & Optimal MinMax & Optimal Cost\\
\hline
eil51 & 51 & 2 & - & - \\ 
eil51 & 51 & 3 & - & - \\ 
berlin52 & 52 & 2 & - & - \\ 
berlin52 & 52 & 3 & - & - \\ 
eil76 & 76 & 2 & - & - \\ 
eil76 & 76 & 3 & - & - \\ 
rat99 & 99 & 2 & - & - \\ 
rat99 & 99 & 3 & - & - \\ 
\hline
\end{tabular}
\caption{Value results after running KMeans with Simulated Annealing}
\label{table:1}
\end{table}

\subsection{Ant Colony Optimization with clustering}
For the Ant Colony Optimization the following best parameters were chosen:
\begin{itemize}
  \item pheromone accepting rate $\alpha$ = 0.8
  \item heuristic accepting rate $\beta$ = 2.5
  \item pheromone evaporation rate $\rho$ = 0.5
  \item number of ants in the colony: 100
  \item number of generations: 100
\end{itemize}

\begin{table}[h!]
\centering
\begin{tabular}{ |c|c|c|c|c| } 
\hline
Instance Name & n & m & Optimal MinMax & Optimal Cost\\
\hline
eil51 & 51 & 2 & - & - \\ 
eil51 & 51 & 3 & - & - \\ 
berlin52 & 52 & 2 & - & - \\ 
berlin52 & 52 & 3 & - & - \\ 
eil76 & 76 & 2 & - & - \\ 
eil76 & 76 & 3 & - & - \\ 
rat99 & 99 & 2 & - & - \\ 
rat99 & 99 & 3 & - & - \\ 
\hline
\end{tabular}
\caption{Value results after running KMeans with Ant Colony Optimization}
\label{table:2}
\end{table}

\subsection{Benchmark}

\begin{table}[h!]
\centering
\begin{tabular}{ |c|c|c|c|c| } 
\hline
Instance Name & n & m & Optimal MinMax & Optimal Cost\\
\hline
eil51 & 51 & 2 & 222.73 & 444.33 \\ 
eil51 & 51 & 3 & [150.70, 159.57] & 477.15 \\ 
berlin52 & 52 & 2 & [4049.05, 4110.21] & 8217.94 \\ 
berlin52 & 52 & 3 & [2753.63, 3244.37] & 9591.15 \\ 
eil76 & 76 & 2 & 280.85 & 561.48 \\ 
eil76 & 76 & 3 & [186.34, 197.34] & 587.65 \\ 
rat99 & 99 & 2 & [620.99, 728.71] & 1456.95 \\ 
rat99 & 99 & 3 & [426.25, 597.17] & 1780.48  \\ 
\hline
\end{tabular}
\caption{Our benchmark, see References[3]}
\label{table:3}
\end{table}

\section{Conclusion}
blabla

\bibliographystyle{plain}
\begin{thebibliography}{9}
\bibitem{} 
TSPLib for data points
\\\url{http://comopt.ifi.uni-heidelberg.de/software/TSPLIB95/tsp/}

\bibitem{} 
2opt method for generating neighbours in Simulated Annealing
\\\url{https://en.wikipedia.org/wiki/2-opt}

\bibitem{} 
Breaban Mihaela, Raluca Necula, Madalina Raschip for benchmark results
\\\url{https://profs.info.uaic.ro/~mtsplib/MinMaxMTSP/index.html}

\bibitem{} 
Marco Dorigo, Thomas St√ºtzle, "Ant Colony Optimization", 2004
\\\url{https://web2.qatar.cmu.edu/~gdicaro/15382/additional/aco-book.pdf}

\bibitem{} 
Breaban Mihaela, Nature Inspired Methods class course, 2020
\\\url{https://profs.info.uaic.ro/~pmihaela/MOC/trajectory.html}

\bibitem{} 
MIT Open Course Ware, "Multidisciplinary System Design Optimization", Olivier de Weck, 2010
\\\url{https://ocw.mit.edu/courses/institute-for-data-systems-and-society/ids-338j-multidisciplinary-system-design-optimization-spring-2010/lecture-notes/MITESD_77S10_lec10.pdf}

\end{thebibliography}
\end{document}
